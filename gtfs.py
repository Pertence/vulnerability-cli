from collections import defaultdict
import geopandas as gpd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
import mplleaflet
import networkx as nx
import numpy as np
from operator import itemgetter
import pandas as pd
import partridge as ptg
import pyproj
from sklearn import model_selection, metrics, preprocessing, ensemble
from scipy.spatial import distance
from scipy import stats
import sys

ecef = pyproj.Proj(proj='geocent', ellps='WGS84', datum='WGS84')
lla = pyproj.Proj(proj='latlong', ellps='WGS84', datum='WGS84')

def create_network(feed):
    G = nx.Graph()
    paths = defaultdict(list)
    for i, s in feed.stop_times.iterrows():
        paths[s['trip_id']].append(s['stop_id'])
    G.add_nodes_from(feed.stop_times['stop_id'])
    for k, v in paths.items():
        nx.add_path(G, v)
    coor = {}
    for i,s in feed.stops.iterrows():
        coor[s['stop_id']] = (s['stop_lon'], s['stop_lat'])
    nx.set_node_attributes(G, coor, 'pos')
    return G

def update_weight_network(G):
    pos = nx.get_node_attributes(G, 'pos')
    stops_xyz = {k:pyproj.transform(lla, ecef, v[0], v[1] , 0, radians=False) for k, v in pos.items()} 
    weight = {e:distance.euclidean(stops_xyz[e[0]], stops_xyz[e[1]]) for e in list(G.edges())}
    nx.set_edge_attributes(G, weight, 'weight')
    return G

def update_network_metrics(G):
    betweenness = nx.betweenness_centrality(G, weight='weight')
    nx.set_node_attributes(G, betweenness, 'betweenness')
    closeness = nx.closeness_centrality(G, distance='weight')
    nx.set_node_attributes(G, closeness, 'closeness')
    clustering = nx.clustering(G, weight='weight')
    nx.set_node_attributes(G, clustering, 'clustering')
    return G

def update_distance_metric(G, sel):
    pos = nx.get_node_attributes(G, 'pos')
    sel_pos = [(g.xy[0][0], g.xy[1][0]) for g in sel['geometry']]
    stops_xyz = {k:pyproj.transform(lla, ecef, v[0], v[1] , 0, radians=False) for k, v in pos.items()} 
    sel_xyz = [pyproj.transform(lla, ecef, v[0], v[1] , 0, radians=False) for v in sel_pos]
    hdist = {k: stats.hmean(distance.cdist(np.array(v).reshape(1, -1), sel_xyz).flatten()) for k, v in stops_xyz.items()}
    nx.set_node_attributes(G, hdist, 'hdist')
    return G

def relative_lcc(G, n):
    components = [len(C) for C in nx.connected_components(G)]
    return max(components)/n

def local_efficiency(G, n):
    return nx.local_efficiency(G)

def assortativity(G, n):
    return nx.degree_assortativity_coefficient(G)

def calculate_metric_assimetry(G, sort, metric):
    d = 20
    n = len(G.nodes)
    p = int(n/d)-5
    metric_list=[]
    for x in range(0, d+1):
        metric_list.append(metric(G, n))
        sample = sort[-p:]
        G.remove_nodes_from(sample)
        sort = sort[:-p]
    return metric_list

def regression_analysis(G):
    distance = nx.get_node_attributes(G, 'hdist')
    betweenness = nx.get_node_attributes(G, 'betweenness')
    closeness = nx.get_node_attributes(G, 'closeness')
    clustering = nx.get_node_attributes(G, 'clustering')
    
    data = [[betweenness[i], closeness[i], clustering[i], distance[i]] for i in G.nodes()]
    df = pd.DataFrame(data, columns=['betweenness', 'closeness', 'clustering', 'distance'])
    print(df.corr())
    
    X = df[['betweenness','closeness', 'clustering']]
    y = df[['distance']]
    
    scaled_X = preprocessing.StandardScaler().fit_transform(X)
    scaled_y = preprocessing.StandardScaler().fit_transform(y)
    
    regr = ensemble.RandomForestRegressor()
    X_train, X_test, y_train, y_test = model_selection.train_test_split(
        scaled_X, scaled_y.ravel(), test_size=0.33, random_state=1)
    regr.fit(X_train, y_train)
    y_pred = regr.predict(X_test)
    
    print(metrics.r2_score(y_test, y_pred))
    print(metrics.mean_squared_error(y_test, y_pred))
    print(regr.feature_importances_)
    return
    
def visualize_assimetry(metric_list, marker, color, label):
    plt.xticks(ticks=np.arange(0,22,2), labels=np.arange(0, 110,10))
    plt.plot(metric_list, marker=marker, color=color, label=label)
    return

def visualize_network(G, metric_list):
    pos = nx.get_node_attributes(G, 'pos')
    plt.figure()
    plt.axis('equal')
    nx.draw(G, pos=pos, node_size=1, alpha=0.7)
    plt.savefig('network.png')
    return

def visualize_top_network(G, sorted_metric):
    pos = nx.get_node_attributes(G, 'pos')
    p = int(len(G.nodes())*0.01)
    sorted_pos = [[pos[k] for k in sorted_metric[0][-p:]],
                  [pos[k] for k in sorted_metric[1][-p:]],
                  [pos[k] for k in sorted_metric[2][-p:]]]
    coor = [list(map(list, zip(*sorted_pos[0]))),
            list(map(list, zip(*sorted_pos[1]))),
            list(map(list, zip(*sorted_pos[2])))]
    print(p)
    plt.figure()
    plt.scatter(coor[0][0], coor[0][1], s=100, color='b', marker='o', alpha=0.5)
    plt.scatter(coor[1][0], coor[1][1], s=100, color='g', marker='s', alpha=0.5)
    plt.scatter(coor[2][0], coor[2][1], s=200, color='r', marker='^', alpha=0.5)
    mplleaflet.show()
    return

def visualize_metrics(metric_list):
    plt.figure()
    scaled_metric = []
    weights = []
    bins = np.arange(0, 1.1, 0.1)
    for i in range(0, len(metric_list)):
        scaled_metric.append([j/max(metric_list[i]) for j in metric_list[i]])
    weights = [np.ones_like(scaled_metric[0])/float(len(scaled_metric[0])), 
               np.ones_like(scaled_metric[1])/float(len(scaled_metric[1])),
               np.ones_like(scaled_metric[2])/float(len(scaled_metric[2]))]
    plt.hist([scaled_metric[0], scaled_metric[1], scaled_metric[2]], bins=bins, weights=weights, rwidth=0.9, color=['b', 'g', 'r'])
    print(np.histogram(scaled_metric[1], bins=bins))
    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))
    plt.gca().xaxis.set_major_formatter(PercentFormatter(1))
    plt.xticks(bins)
    plt.yticks(bins)
    plt.savefig('histogram.png')
    return

def skewness(metric_list):
    plt.figure()
    legend = [round(stats.skew(metric_list[0], bias=False), 4), round(stats.skew(metric_list[1], bias=False), 4), round(stats.skew(metric_list[2], bias=False), 4)]
    visualize_assimetry(metric_list[0], color='b', marker='o', label=f"Between.={legend[0]}")
    visualize_assimetry(metric_list[1], color='g', marker='s', label=f"Close.={legend[1]}")
    visualize_assimetry(metric_list[2], color='r', marker='^', label=f"Cluster.={legend[2]}")
    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=3, mode="expand", borderaxespad=0.)
    return
 
def main():
    name = sys.argv[1]
    
    #Network creation and metrics calculation
    feed = ptg.load_feed('data/'+name+'.zip')
    print('File read')
    G = create_network(feed)
    print('Network created')
    G1 = update_weight_network(G)
    print('Network updated')
    G2 = update_network_metrics(G1)
    print('Network metrics updated')
    nx.write_gml(G2, name+'.gml')
    
    G = nx.read_gml('data/'+name+'.gml')
    
    #Visualize metrics
    betweenness = nx.get_node_attributes(G, 'betweenness')
    closeness = nx.get_node_attributes(G, 'closeness')
    clustering = nx.get_node_attributes(G, 'clustering')
    sorted_list = [list(dict(sorted(betweenness.items(), key=itemgetter(1))).keys()),
                   list(dict(sorted(closeness.items(), key=itemgetter(1))).keys()),
                   list(dict(sorted(clustering.items(), key=itemgetter(1))).keys())]
    visualize_top_network(G, sorted_list)
    visualize_metrics([list(betweenness.values()),
                       list(closeness.values()),
                       list(clustering.values())])

    #Assimetry calculation
    metric = local_efficiency
    efficiency_list = []
    for i in range(0, len(sorted_list)):
        efficiency_list.append(calculate_metric_assimetry(G.copy(), sorted_list[i], metric))
    skewness(efficiency_list)
    plt.xlabel("Percentage of Removed Nodes(%)")
    plt.ylabel("Efficiency")
    plt.savefig('efficiency.png')
    
    metric = relative_lcc
    lcc_list = []
    for i in range(0, len(sorted_list)):
        lcc_list.append(calculate_metric_assimetry(G.copy(), sorted_list[i], metric))
    skewness(lcc_list)
    plt.xlabel("Percentage of Removed Nodes(%)")
    plt.ylabel("Relative Largest Connected Component")
    plt.savefig('lcc.png')
    
    return
  
if __name__== "__main__":
    main()
